{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: Dependency Parsing\n",
    "\n",
    "### Estimated Time: ~10 hours\n",
    "\n",
    "This assignment will build a neural dependency parser using PyTorch.  In part 1, we will review two general neural network techniques (Adam optimization and Dropout).  In part 2, we will implement and train a dependency parser using techniques from part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: https://gitlab.com/vojtamolda/stanford-cs224n-nlp-with-dl/-/blob/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.  Adam Optimization and Dropout\n",
    "\n",
    "### a) Adam\n",
    "\n",
    "Recall the SGD update rule:\n",
    "\n",
    "$$\\theta = \\theta - \\alpha\\triangledown_\\theta J_{\\text{minibatch}}(\\theta)$$\n",
    "\n",
    "where $\\theta$ is a vector containing all of the model parameters, $J$ is the loss function, $\\triangledown_\\theta J_{\\text{minibatch}}(\\theta)$ is the gradient of the loss function, and $\\alpha$ is the learning rate.  Adam is another possible update rule with two additional steps.\n",
    "\n",
    "- (2 pts) First, Adam uses a trick called momentum by keep track of $\\mathbf{m}$, a rolling average of the gradients:\n",
    "\n",
    "$$\\mathbf{m} = \\beta_1 \\mathbf{m} + (1-\\beta_1)\\triangledown_\\theta J_{\\text{minibatch}}(\\theta)$$\n",
    "$$\\theta = \\theta - \\alpha \\mathbf{m}$$\n",
    "\n",
    "  where $\\beta_1$ is a hyperparameter between 0 and 1 (often set to 0.9).  Briefly explain in 2-4 sentences (just give an intuition) how using $\\mathbf{m}$ stops the updates from varying as much and why this low variance may be helpful to learning, overall.\n",
    "  \n",
    "#### <font color=\"red\">Write your answer here.</font> \n",
    "\n",
    "SGD uses mini-batches instead of the entire dataset to calculate the gradient for updating the weights, so the\n",
    "updates suffer from noise. \n",
    "Momentum accumulates updates over a time steps into recent\n",
    "weighted average to update the weights. The trajectory the weights follow through their vector space therefore becomes less\n",
    "random and more closely follows the true downward direction of the cost function.\n",
    "\n",
    "- (2 pts) Adam extends the idea of momentum with the trick of adaptive learning rates by keep track of $\\mathbf{v}$, a rolling average of the magnitudes of the gradients:\n",
    "\n",
    "$$\\mathbf{m} = \\beta_1 \\mathbf{m} + (1 - \\beta_1)\\triangledown_\\theta J_{\\text{minibatch}}(\\theta)$$\n",
    "$$\\mathbf{v} = \\beta_2 \\mathbf{v} + (1 - \\beta_2)(\\triangledown_\\theta J_{\\text{minibatch}}(\\theta) \\circ \\triangledown_\\theta J_{\\text{minibatch}}(\\theta))$$\n",
    "$$\\theta = \\theta - \\alpha \\mathbf{m} \\mathbin{/} \\sqrt{\\mathbf{v}}$$\n",
    "\n",
    "where $\\circ$ and $\\mathbin{/}$ denote elementwise multiplication and division (not dot product!).  $\\beta_2$ is a hyperparameter between 0 and 1 (often set to 0.99).  Since Adam divides the update by $\\sqrt{\\mathbf{v}}$, what kinds of weights will receive larger update and smaller update?  Give some simple example of how.  Why might this help with learning?\n",
    "\n",
    "#### <font color=\"red\">Write your answer here.</font> \n",
    "Adam's weighting method gives larger effective learning rates to weights that have recently received a smaller\n",
    "updates. The division by the element-wise square root serves as a normalizer that treats all weights equally. To a large extend\n",
    "each individual weight is given an update of roughly equal magnitude irrespectively of the size of it's derivative.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Dropout\n",
    "\n",
    "Dropout is a regularization technique.  During training, dropout randomly sets units in the hidden layer $\\mathbf{h}$ to zero with probabilty $p_{\\text{drop}}$ (dropping different units each minibatch), and then multiplies $\\mathbf{h}$ by a constant $\\gamma$.  We can write this as:\n",
    "\n",
    "$$\\mathbf{h}_{\\text{drop}} = \\gamma \\mathbf{d} \\circ \\mathbf{h}$$\n",
    "\n",
    "where $\\mathbf{d} \\in \\{0, 1\\}^{D_h}$ ($D_h$ is the size of $\\mathbf{h}$) is a mask vector where each entry is 0 with probability $p_{\\text{drop}}$ and 1 with probability ($1 - p_{\\text{drop}}$). For the gamma constant, $\\gamma$ is chosen such that the expected value of $\\mathbf{h}_{\\text{drop}}$ is $\\mathbf{h}$ \n",
    "\n",
    "$$\\mathbb{E}_{\\text{p_drop}}[\\mathbf{h}_{\\text{drop}}]_i = h_i$$\n",
    "\n",
    "for all $i \\in \\{1, \\cdots, D_h\\}$\n",
    "\n",
    "- (2 pts) What must $\\gamma$ equal in term of $p_\\text{drop}$?  Briefly justify your answers or show your math derivation using the equations given above.\n",
    "\n",
    "#### <font color=\"red\">Write your answer here.</font> \n",
    "\n",
    "b) i. The expected magnitude of the output vector with and without the dropout active should stay the same. If the\n",
    "following layers would receive input vectors of a smaller magnitude the non-linearities like ReLU wouldn't operate in the\n",
    "correct region and the prediction wouldn't be correct.\n",
    "\n",
    "- (2pts) Why shoup dropout be applied only during training? Why should dropout NOT be applied during evaluation?\n",
    "\n",
    "#### <font color=\"red\">Write your answer here.</font> \n",
    "\n",
    "ii. Dropout regularization works in a way that the model doesnt start to depend on a single classifier , node too much. Each classifier is train to some extent separately and learns a different aspect of the problem. If we didnt turn it off during evaluation we would not be using this 'knowledge' that all the classifiers have gained by turning the other nodes off.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.  Neural Transition-Based Dependency Parsing\n",
    "\n",
    "We will be implementing a neural dependency parser with the goal of maximizing the performance on the UAS (Unlabeled Attachment Score) metric.\n",
    "\n",
    "A dependency parser analyzes the grammatical structure of a sentence, establishing relationships between head words, and words which modify those heads.  There are multiple types of dependency parsers, including transition-based parsers, graph-based parsers, and feature-based parsers.  Your implementation will be a transition-based parser, which incrementally builds up a parse one step at a time.  At every step, it maintains a partial parse, which is represented as follows:\n",
    "\n",
    "- A **stack** of words that are currently being processed\n",
    "- A **buffer** of words yet to be processed.\n",
    "- A **list of dependencies** predicted by the parser\n",
    "\n",
    "Initially, the stack only contains ROOT, the dependencies list is empty, and the buffer contains all words of the sentence in order. At each step, the parser applies a tranistion to the partial parse until its buffer is empty and the stack size is 1.  The following transitions can be applied:\n",
    "\n",
    "- $\\texttt{SHIFT}$: removes the first word from the buffer and pushes it onto the stack.\n",
    "- $\\texttt{LEFTARC}$: marks the second (second msot recently aded) item on the stack as a dependent of the first item and removes the second item from the stack, adding a *first_word* $\\rightarrow$ *second_word* dependency to the dependeny list.\n",
    "- $\\texttt{RIGHTARC}$: marks the first (second msot recently aded) item on the stack as a dependent of the second item and removes the first item from the stack, adding a *second_word* $\\rightarrow$ *first_word* dependency to the dependeny list.\n",
    "\n",
    "On each step, your parser will decide among the three transitions using a neural network classifier.\n",
    "\n",
    "- (4 pts) Go through the sequence of transitions needed for parsing the sentence *I parsed this sentence correctly*.  The dependency tree for the sentence is shown below.  At each step, give the configuration of the stack and buffer, as well as what transition was applied this step and what new dependency was added (if any).  The first three steps are provided below as an example.\n",
    "\n",
    "<!-- <img src = \"/img/parsetree.png\" width=400> -->\n",
    "\n",
    "| Stack | Buffer | New dependency | Transition |\n",
    "| :--   |  :--   | :--            | :--        |\n",
    "| [ROOT] | [I, parsed, this, sentence, correctly] | | Init |\n",
    "| [ROOT, I] | [parsed, this, sentence, correctly] | | SHIFT |\n",
    "| [ROOT, I, parsed] | [this, sentence, correctly] | | SHIFT|\n",
    "| [ROOT, parsed] | [this, sentence, correctly] | parsed $\\rightarrow$ I | LEFTARC |\n",
    "|  |  | |  |\n",
    "| [ROOT, parsed, this] | [sentence, correctly] | | SHIFT |\n",
    "| [ROOT, parsed, this, sentence] | [correctly] | | SHIFT |\n",
    "| [ROOT, parsed, sentence ] | [correctly] | sentence $\\rightarrow$ this | LEFTARC |\n",
    "| [ROOT, parsed] | [correctly] | parsed $\\rightarrow$ sentence | RIGHTARC |\n",
    "| [ROOT, parsed, correctly] | [] |  | SHIFT |\n",
    "| [ROOT, parsed] | [] | parsed $\\rightarrow$ correctly | RIGHTARC |\n",
    "| [ROOT] | [] | ROOT $\\rightarrow$ parsed | RIGHTARC |\n",
    "\n",
    "#### <font color=\"red\">Write your answer here.</font> \n",
    "\n",
    "- (2 pts) A sentence containing $n$ words will be parsed in how many steps (in terms of $n$)?  Briefly explain in 1-2 sentences why.\n",
    "\n",
    "#### <font color=\"red\">Write your answer here.</font> \n",
    "\n",
    "A sentence with n words will be parsed in 2n steps excluding the initial configuration. Every word in the buffer needs to be eventually put on the stack\n",
    "which takes n steps. Eventually each word has to be removed from the stack to form a dependency which takes another n steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:  Parser\n",
    "\n",
    "- (6pts) Implement the <code>\\_\\_init\\_\\_</code> and <code>parse_step</code> functions in the <code>PartialParse</code> class below.  This implements the transition mechanics your parser will use.  \n",
    "\n",
    "Test your function by running <code>test_parse_step()</code> followed by <code>test_parse()</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (8pts) Our network will predict which transition should be applied next to a partial parse.  We could use it to parse a single sentence by applying predicted transitions until the parse is complete.  However, neural networks run much more efficiently when making predictions about batches of data (i.e., predicting the next transition for any different partial parses simultaneously).  We can parse sentences in minibatches with the following algorithm\n",
    "\n",
    "**Input**: <code>sentences</code>, a list of sentences to be parsed and <code>model</code>, our model that makes parse decisions\n",
    "\n",
    "1. Initialize <code>partial_parses</code> as a list of <code>PartialParses</code>, one for each sentence in <code>sentences</code>\n",
    "2. Initailize <code>unfinished_parses</code> as a shallow copy of <code>partial_parses</code>\n",
    "3. **while** <code>unfinished_parses</code> is not empty **do**\n",
    "    - Take the first <code>batch_size</code> parses in <code>unfinished_parses</code> as a minibatch\n",
    "    - Use the <code>model</code> to predict the next transition for each partial parse in the minitbatch\n",
    "    - Peform a parse step on each partial parse in the minibatch with its predicted transition\n",
    "    - Remove the completed (empty buffer and stack of size 1) parses from <code>unfinished_parses</code>\n",
    "    \n",
    "**Return**: The dependencies for each parse in <code>partial_parses</code>\n",
    "\n",
    "Implement this algorithm in the <code>minibatch_parse</code> function below.\n",
    "\n",
    "Test your function by running <code>test_minibatch_parse()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartialParse(object):\n",
    "    def __init__(self, sentence):\n",
    "        \"\"\"Initializes this partial parse.\n",
    "        @param sentence (list of str): The sentence to be parsed as a list of words.\n",
    "                                        Your code should not modify the sentence.\n",
    "        \"\"\"\n",
    "        # The sentence being parsed is kept for bookkeeping purposes. Do not alter it in your code.\n",
    "        self.sentence = sentence\n",
    "\n",
    "        # YOUR CODE HERE (3 Lines)\n",
    "        # Your code should initialize the following fields:\n",
    "        # self.stack: The current stack represented as a list with the top of the stack as the\n",
    "        # last element of the list.\n",
    "        # self.buffer: The current buffer represented as a list with the first item on the\n",
    "        # buffer as the first item of the list\n",
    "        # self.dependencies: The list of dependencies produced so far. Represented as a list of\n",
    "        # tuples where each tuple is of the form (head, dependent).\n",
    "        # Order for this list doesn't matter.\n",
    "        ###\n",
    "        # Note: The root token should be represented with the string \"ROOT\"\n",
    "        ###\n",
    "\n",
    "\n",
    "        self.stack = [\"ROOT\"]\n",
    "        self.buffer = sentence[:]\n",
    "        self.dependencies = []               \n",
    "        \n",
    "        \n",
    "\n",
    "        # END YOUR CODE\n",
    "\n",
    "    def parse_step(self, transition):\n",
    "        \"\"\"Performs a single parse step by applying the given transition to\n",
    "           this partial parse.\n",
    "        @param transition (str): A string that equals \"S\", \"LA\", or \"RA\"\n",
    "                                 representing the shift, left-arc, and right-arc\n",
    "                                 transitions. You can assume the provided\n",
    "                                 transition is a legal transition.\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE (~7-10 Lines)\n",
    "        # TODO:\n",
    "        # Implement a single parsing step, i.e. the logic for the following as\n",
    "        # described in the pdf handout:\n",
    "        # 1. Shift\n",
    "        # 2. Left Arc\n",
    "        # 3. Right Arc\n",
    "\n",
    "        if transition == \"S\":\n",
    "            self.stack += [self.buffer.pop(0)]\n",
    "        else:\n",
    "            l = self.stack[-1]\n",
    "            p = self.stack[-2]\n",
    "            if transition == \"RA\":\n",
    "                self.dependencies += [(p, l)]\n",
    "                self.stack.pop(-1)\n",
    "            if transition == \"LA\":\n",
    "                self.dependencies += [(l, p)]\n",
    "                self.stack.pop(-2)      \n",
    "        \n",
    "        # END YOUR CODE\n",
    "\n",
    "    def parse(self, transitions):\n",
    "        \"\"\"Applies the provided transitions to this PartialParse\n",
    "        @param transitions (list of str): The list of transitions in the order\n",
    "                                          they should be applied.\n",
    "        @return dsependencies (list of string tuples): The list of dependencies\n",
    "                                                       produced when parsing the\n",
    "                                                       sentence. Represented as\n",
    "                                                       a list of tuples where each\n",
    "                                                       tuple is of the form\n",
    "                                                       (head, dependent).\n",
    "        \"\"\"\n",
    "        for transition in transitions:\n",
    "            self.parse_step(transition)\n",
    "        return self.dependencies\n",
    "\n",
    "    def is_completed(self):\n",
    "        return (len(self.buffer) == 0) and (len(self.stack) == 1)\n",
    "\n",
    "\n",
    "def minibatch_parse(sentences, model, batch_size):\n",
    "    \"\"\"Parses a list of sentences in minibatches using a model.\n",
    "    @param sentences (list of list of str): A list of sentences to be parsed\n",
    "                                            (each sentence is a list of words\n",
    "                                            and each word is of type string)\n",
    "    @param model (ParserModel): The model that makes parsing decisions. It is\n",
    "                                assumed to have a function\n",
    "                                model.predict(partial_parses) that takes in a\n",
    "                                list of PartialParses as input and\n",
    "                                returns a list of transitions predicted for each\n",
    "                                parse. That is, after calling\n",
    "                                    transitions = model.predict(partial_parses)\n",
    "                                transitions[i] will be the next transition to\n",
    "                                apply to partial_parses[i].\n",
    "    @param batch_size (int): The number of PartialParses to include in each minibatch\n",
    "    @return dependencies (list of dependency lists): A list where each element\n",
    "                                                     is the dependencies list for\n",
    "                                                     parsed sentence. Ordering\n",
    "                                                     should be the same as in\n",
    "                                                     sentences (i.e.,\n",
    "                                                     dependencies[i] should\n",
    "                                                     contain the parse for\n",
    "                                                     sentences[i]).\n",
    "    \"\"\"\n",
    "    dependencies = []\n",
    "\n",
    "    # YOUR CODE HERE (~8-10 Lines)\n",
    "    # TODO:\n",
    "    # Implement the minibatch parse algorithm\n",
    "    ###\n",
    "    # Note: A shallow copy can be made with the \"=\" sign\n",
    "    # in python, e.g. unfinished_parses = partial_parses[:].\n",
    "    # Here `unfinished_parses` is a shallow copy of `partial_parses`.\n",
    "    # In Python, a shallow copied list like `unfinished_parses` does not contain\n",
    "    # new instances of the object stored in `partial_parses`. Rather both lists\n",
    "    # refer to the same objects.\n",
    "    # In our case, `partial_parses` contains a list of partial parses.\n",
    "    # `unfinished_parses` contains references to the same objects. Thus, you\n",
    "    # should NOT use the `del` operator to remove objects from the\n",
    "    # `unfinished_parses` list. This will free the underlying memory that\n",
    "    # is being accessed by `partial_parses` and may cause your code to crash.\n",
    "\n",
    "    \n",
    "    partial_parses = [PartialParse(sentence) for sentence in sentences]\n",
    "    unfinished_parses = partial_parses[:]\n",
    "\n",
    "    while unfinished_parses:\n",
    "        minibatch = unfinished_parses[:batch_size]\n",
    "        transitions = model.predict(minibatch)\n",
    "\n",
    "        for partial_parse, transition in zip(minibatch, transitions):\n",
    "            partial_parse.parse_step(transition)\n",
    "\n",
    "        unfinished_parses = [partial_parse for partial_parse in partial_parses\n",
    "                             if partial_parse.buffer or len(partial_parse.stack) != 1]\n",
    "\n",
    "    dependencies = [partial_parse.dependencies for partial_parse in partial_parses]  \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return dependencies\n",
    "\n",
    "\n",
    "def test_step(name, transition, stack, buf, deps,\n",
    "              ex_stack, ex_buf, ex_deps):\n",
    "    \"\"\"Tests that a single parse step returns the expected output\"\"\"\n",
    "    pp = PartialParse([])\n",
    "    pp.stack, pp.buffer, pp.dependencies = stack, buf, deps\n",
    "\n",
    "    pp.parse_step(transition)\n",
    "    stack, buf, deps = (tuple(pp.stack), tuple(pp.buffer),\n",
    "                        tuple(sorted(pp.dependencies)))\n",
    "    assert stack == ex_stack, \\\n",
    "        \"{:} test resulted in stack {:}, expected {:}\".format(\n",
    "            name, stack, ex_stack)\n",
    "    assert buf == ex_buf, \\\n",
    "        \"{:} test resulted in buffer {:}, expected {:}\".format(\n",
    "            name, buf, ex_buf)\n",
    "    assert deps == ex_deps, \\\n",
    "        \"{:} test resulted in dependency list {:}, expected {:}\".format(\n",
    "            name, deps, ex_deps)\n",
    "    print(\"{:} test passed!\".format(name))\n",
    "\n",
    "\n",
    "def test_parse_step():\n",
    "    \"\"\"Simple tests for the PartialParse.parse_step function\n",
    "    Warning: these are not exhaustive\n",
    "    \"\"\"\n",
    "    test_step(\"SHIFT\", \"S\", [\"ROOT\", \"the\"], [\"cat\", \"sat\"], [],\n",
    "              (\"ROOT\", \"the\", \"cat\"), (\"sat\",), ())\n",
    "    test_step(\"LEFT-ARC\", \"LA\", [\"ROOT\", \"the\", \"cat\"], [\"sat\"], [],\n",
    "              (\"ROOT\", \"cat\",), (\"sat\",), ((\"cat\", \"the\"),))\n",
    "    test_step(\"RIGHT-ARC\", \"RA\", [\"ROOT\", \"run\", \"fast\"], [], [],\n",
    "              (\"ROOT\", \"run\",), (), ((\"run\", \"fast\"),))\n",
    "\n",
    "\n",
    "def test_parse():\n",
    "    \"\"\"Simple tests for the PartialParse.parse function\n",
    "    Warning: these are not exhaustive\n",
    "    \"\"\"\n",
    "    sentence = [\"parse\", \"this\", \"sentence\"]\n",
    "    dependencies = PartialParse(sentence).parse(\n",
    "        [\"S\", \"S\", \"S\", \"LA\", \"RA\", \"RA\"])\n",
    "    dependencies = tuple(sorted(dependencies))\n",
    "    expected = (('ROOT', 'parse'), ('parse', 'sentence'), ('sentence', 'this'))\n",
    "    assert dependencies == expected,  \\\n",
    "        \"parse test resulted in dependencies {:}, expected {:}\".format(\n",
    "            dependencies, expected)\n",
    "    assert tuple(sentence) == (\"parse\", \"this\", \"sentence\"), \\\n",
    "        \"parse test failed: the input sentence should not be modified\"\n",
    "    print(\"parse test passed!\")\n",
    "\n",
    "class DummyModel(object):\n",
    "    \"\"\"Dummy model for testing the minibatch_parse function\n",
    "    First shifts everything onto the stack and then does exclusively right arcs if the first word of\n",
    "    the sentence is \"right\", \"left\" if otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    def predict(self, partial_parses):\n",
    "        return [(\"RA\" if pp.stack[1] == \"right\" else \"LA\") if len(pp.buffer) == 0 else \"S\"\n",
    "                for pp in partial_parses]\n",
    "\n",
    "\n",
    "def test_dependencies(name, deps, ex_deps):\n",
    "    \"\"\"Tests the provided dependencies match the expected dependencies\"\"\"\n",
    "    deps = tuple(sorted(deps))\n",
    "    assert deps == ex_deps, \\\n",
    "        \"{:} test resulted in dependency list {:}, expected {:}\".format(\n",
    "            name, deps, ex_deps)\n",
    "\n",
    "\n",
    "def test_minibatch_parse():\n",
    "    \"\"\"Simple tests for the minibatch_parse function\n",
    "    Warning: these are not exhaustive\n",
    "    \"\"\"\n",
    "    sentences = [[\"right\", \"arcs\", \"only\"],\n",
    "                 [\"right\", \"arcs\", \"only\", \"again\"],\n",
    "                 [\"left\", \"arcs\", \"only\"],\n",
    "                 [\"left\", \"arcs\", \"only\", \"again\"]]\n",
    "    deps = minibatch_parse(sentences, DummyModel(), 2)\n",
    "    test_dependencies(\"minibatch_parse\", deps[0],\n",
    "                      (('ROOT', 'right'), ('arcs', 'only'), ('right', 'arcs')))\n",
    "    test_dependencies(\"minibatch_parse\", deps[1],\n",
    "                      (('ROOT', 'right'), ('arcs', 'only'), ('only', 'again'), ('right', 'arcs')))\n",
    "    test_dependencies(\"minibatch_parse\", deps[2],\n",
    "                      (('only', 'ROOT'), ('only', 'arcs'), ('only', 'left')))\n",
    "    test_dependencies(\"minibatch_parse\", deps[3],\n",
    "                      (('again', 'ROOT'), ('again', 'arcs'), ('again', 'left'), ('again', 'only')))\n",
    "    print(\"minibatch_parse test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function by running <code>test_parse_step()</code> followed by <code>test_parse()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHIFT test passed!\n",
      "LEFT-ARC test passed!\n",
      "RIGHT-ARC test passed!\n"
     ]
    }
   ],
   "source": [
    "#testing your parse_step\n",
    "#turn on when you are ready\n",
    "test_parse_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function by running <code>test_minibatch_parse()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minibatch_parse test passed!\n"
     ]
    }
   ],
   "source": [
    "#testing your minibatch_parse\n",
    "#turn on when you are ready\n",
    "test_minibatch_parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Neural Network\n",
    "\n",
    "Let's train a neural network to predict, given the state of the stack, buffer, and dependencies, which transition should be applied next.\n",
    "\n",
    "First, the model extracts a feature vector representing the current state.  We will be using the feature set presented in *A Fast and Accurate Dependency Parser using Neural Networks (Chen and Manning 2014)*.  The function extracting these features are implemented for you here below.\n",
    "\n",
    "This feature vector consists of a list of tokens (e.g., the last work in the stack, first word in the buffer, dependent of the second-to-last word in the stack if there is one, etc.). They can be represented as a list of integers $\\mathbf{w} = [w_1, w_2, \\cdots, w_m]$ where $m$ is the number of features and each $0 \\leq w_i \\leq |V|$ is the index of a token in the vocabulary ($|V|$ is the vocabulary size).  Then our network looks up an embedding for each word and concatenates them into a single input vector:\n",
    "\n",
    "$$\\mathbf{x} = [\\mathbf{E}_{w_1}, \\cdots, \\mathbf{E}_{w_m}] \\in \\mathbb{R}^{dm}$$\n",
    "\n",
    "where $\\mathbf{E} \\in \\mathbb{R}^{|V| \\times d}$ is an embedding matrix with each row $\\mathbf{E}_w$ as the vector for a particular word $w$\n",
    "\n",
    "We then compute our prediction as:\n",
    "\n",
    "$$\\mathbf{h} = \\text{ReLU}(\\mathbf{xW} + \\mathbf{b}_1)$$\n",
    "$$\\mathbf{l} = \\mathbf{hU} + \\mathbf{b}_2$$\n",
    "$$\\hat{\\mathbf{y}} = \\text{softmax}(l)$$\n",
    "\n",
    "where $\\mathbf{h}$ is referred to as the hidden layer, $\\mathbf{l}$ is the logits, $\\hat{\\mathbf{y}}$ is the predictions, and $\\text{ReLU}(z) = \\text{max}(z, 0))$.  We will then train the model to minimize cross-entropy (CE) loss:\n",
    "\n",
    "$$J(\\theta) = \\text{CE}(\\mathbf{y}, \\hat{\\mathbf{y}}) = -\\sum_{i=1}^{3}y_i \\log \\hat{y}_i$$\n",
    "\n",
    "To compute the loss for the training set, we average this $J(\\theta)$ across all training examples.  We will use UAS (Unlabeled Attachment Score) as main metric, which is computed as the ratio between number of correctly predicted dependencies and the number of total dependencies despite of the relations (our model doesn't predict this). \n",
    "\n",
    "Below this code, you will find a skeleton code to implement this network using PyTorch.  Complete the <code>\\_\\_init\\_\\_</code>, <code>embedding_lookup</code> and <code>forward</code> functions to implement the model.  Then complete the <code>train_for_epoch</code> and <code>train</code> functions to actually train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do not change this code; this is provided for you which helps extract features from the stack, buffers and dependencies\"\n",
    "\n",
    "P_PREFIX = '<p>:'\n",
    "L_PREFIX = '<l>:'\n",
    "UNK = '<UNK>'\n",
    "NULL = '<NULL>'\n",
    "ROOT = '<ROOT>'\n",
    "\n",
    "def get_minibatches(data, minibatch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    Iterates through the provided data one minibatch at at time. You can use this function to\n",
    "    iterate through data in minibatches as follows:\n",
    "\n",
    "        for inputs_minibatch in get_minibatches(inputs, minibatch_size):\n",
    "            ...\n",
    "\n",
    "    Or with multiple data sources:\n",
    "\n",
    "        for inputs_minibatch, labels_minibatch in get_minibatches([inputs, labels], minibatch_size):\n",
    "            ...\n",
    "\n",
    "    Args:\n",
    "        data: there are two possible values:\n",
    "            - a list or numpy array\n",
    "            - a list where each element is either a list or numpy array\n",
    "        minibatch_size: the maximum number of items in a minibatch\n",
    "        shuffle: whether to randomize the order of returned data\n",
    "    Returns:\n",
    "        minibatches: the return value depends on data:\n",
    "            - If data is a list/array it yields the next minibatch of data.\n",
    "            - If data a list of lists/arrays it returns the next minibatch of each element in the\n",
    "              list. This can be used to iterate through multiple data sources\n",
    "              (e.g., features and labels) at the same time.\n",
    "\n",
    "    \"\"\"\n",
    "    list_data = type(data) is list and (type(data[0]) is list or type(data[0]) is np.ndarray)\n",
    "    data_size = len(data[0]) if list_data else len(data)\n",
    "    indices = np.arange(data_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    for minibatch_start in np.arange(0, data_size, minibatch_size):\n",
    "        minibatch_indices = indices[minibatch_start:minibatch_start + minibatch_size]\n",
    "        yield [_minibatch(d, minibatch_indices) for d in data] if list_data \\\n",
    "            else _minibatch(data, minibatch_indices)\n",
    "\n",
    "\n",
    "def _minibatch(data, minibatch_idx):\n",
    "    return data[minibatch_idx] if type(data) is np.ndarray else [data[i] for i in minibatch_idx]\n",
    "\n",
    "\n",
    "def test_all_close(name, actual, expected):\n",
    "    if actual.shape != expected.shape:\n",
    "        raise ValueError(\"{:} failed, expected output to have shape {:} but has shape {:}\"\n",
    "                         .format(name, expected.shape, actual.shape))\n",
    "    if np.amax(np.fabs(actual - expected)) > 1e-6:\n",
    "        raise ValueError(\"{:} failed, expected {:} but value is {:}\".format(name, expected, actual))\n",
    "    else:\n",
    "        print(name, \"passed!\")\n",
    "\n",
    "\n",
    "class Config(object):\n",
    "    language = 'english'\n",
    "    with_punct = True\n",
    "    unlabeled = True\n",
    "    lowercase = True\n",
    "    use_pos = True\n",
    "    use_dep = True\n",
    "    use_dep = use_dep and (not unlabeled)\n",
    "    data_path = './data-a3'\n",
    "    train_file = 'train.conll'\n",
    "    dev_file = 'dev.conll'\n",
    "    test_file = 'test.conll'\n",
    "    embedding_file = './data-a3/en-cw.txt'\n",
    "\n",
    "\n",
    "class Parser(object):\n",
    "    \"\"\"Contains everything needed for transition-based dependency parsing except for the model\"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        root_labels = list([l for ex in dataset\n",
    "                           for (h, l) in zip(ex['head'], ex['label']) if h == 0])\n",
    "        counter = Counter(root_labels)\n",
    "        if len(counter) > 1:\n",
    "            logging.info('Warning: more than one root label')\n",
    "            logging.info(counter)\n",
    "        self.root_label = counter.most_common()[0][0]\n",
    "        deprel = [self.root_label] + list(set([w for ex in dataset\n",
    "                                               for w in ex['label']\n",
    "                                               if w != self.root_label]))\n",
    "        tok2id = {L_PREFIX + l: i for (i, l) in enumerate(deprel)}\n",
    "        tok2id[L_PREFIX + NULL] = self.L_NULL = len(tok2id)\n",
    "\n",
    "        config = Config()\n",
    "        self.unlabeled = config.unlabeled\n",
    "        self.with_punct = config.with_punct\n",
    "        self.use_pos = config.use_pos\n",
    "        self.use_dep = config.use_dep\n",
    "        self.language = config.language\n",
    "\n",
    "        if self.unlabeled:\n",
    "            trans = ['L', 'R', 'S']\n",
    "            self.n_deprel = 1\n",
    "        else:\n",
    "            trans = ['L-' + l for l in deprel] + ['R-' + l for l in deprel] + ['S']\n",
    "            self.n_deprel = len(deprel)\n",
    "\n",
    "        self.n_trans = len(trans)\n",
    "        self.tran2id = {t: i for (i, t) in enumerate(trans)}\n",
    "        self.id2tran = {i: t for (i, t) in enumerate(trans)}\n",
    "\n",
    "        # logging.info('Build dictionary for part-of-speech tags.')\n",
    "        tok2id.update(build_dict([P_PREFIX + w for ex in dataset for w in ex['pos']],\n",
    "                                  offset=len(tok2id)))\n",
    "        tok2id[P_PREFIX + UNK] = self.P_UNK = len(tok2id)\n",
    "        tok2id[P_PREFIX + NULL] = self.P_NULL = len(tok2id)\n",
    "        tok2id[P_PREFIX + ROOT] = self.P_ROOT = len(tok2id)\n",
    "\n",
    "        # logging.info('Build dictionary for words.')\n",
    "        tok2id.update(build_dict([w for ex in dataset for w in ex['word']],\n",
    "                                  offset=len(tok2id)))\n",
    "        tok2id[UNK] = self.UNK = len(tok2id)\n",
    "        tok2id[NULL] = self.NULL = len(tok2id)\n",
    "        tok2id[ROOT] = self.ROOT = len(tok2id)\n",
    "\n",
    "        self.tok2id = tok2id\n",
    "        self.id2tok = {v: k for (k, v) in tok2id.items()}\n",
    "\n",
    "        self.n_features = 18 + (18 if config.use_pos else 0) + (12 if config.use_dep else 0)\n",
    "        self.n_tokens = len(tok2id)\n",
    "\n",
    "    def vectorize(self, examples):\n",
    "        vec_examples = []\n",
    "        for ex in examples:\n",
    "            word = [self.ROOT] + [self.tok2id[w] if w in self.tok2id\n",
    "                                  else self.UNK for w in ex['word']]\n",
    "            pos = [self.P_ROOT] + [self.tok2id[P_PREFIX + w] if P_PREFIX + w in self.tok2id\n",
    "                                   else self.P_UNK for w in ex['pos']]\n",
    "            head = [-1] + ex['head']\n",
    "            label = [-1] + [self.tok2id[L_PREFIX + w] if L_PREFIX + w in self.tok2id\n",
    "                            else -1 for w in ex['label']]\n",
    "            vec_examples.append({'word': word, 'pos': pos,\n",
    "                                 'head': head, 'label': label})\n",
    "        return vec_examples\n",
    "\n",
    "    def extract_features(self, stack, buf, arcs, ex):\n",
    "        if stack[0] == \"ROOT\":\n",
    "            stack[0] = 0\n",
    "\n",
    "        def get_lc(k):\n",
    "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] < k])\n",
    "\n",
    "        def get_rc(k):\n",
    "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] > k],\n",
    "                          reverse=True)\n",
    "\n",
    "        p_features = []\n",
    "        l_features = []\n",
    "        features = [self.NULL] * (3 - len(stack)) + [ex['word'][x] for x in stack[-3:]]\n",
    "        features += [ex['word'][x] for x in buf[:3]] + [self.NULL] * (3 - len(buf))\n",
    "        if self.use_pos:\n",
    "            p_features = [self.P_NULL] * (3 - len(stack)) + [ex['pos'][x] for x in stack[-3:]]\n",
    "            p_features += [ex['pos'][x] for x in buf[:3]] + [self.P_NULL] * (3 - len(buf))\n",
    "\n",
    "        for i in range(2):\n",
    "            if i < len(stack):\n",
    "                k = stack[-i-1]\n",
    "                lc = get_lc(k)\n",
    "                rc = get_rc(k)\n",
    "                llc = get_lc(lc[0]) if len(lc) > 0 else []\n",
    "                rrc = get_rc(rc[0]) if len(rc) > 0 else []\n",
    "\n",
    "                features.append(ex['word'][lc[0]] if len(lc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][rc[0]] if len(rc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][lc[1]] if len(lc) > 1 else self.NULL)\n",
    "                features.append(ex['word'][rc[1]] if len(rc) > 1 else self.NULL)\n",
    "                features.append(ex['word'][llc[0]] if len(llc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][rrc[0]] if len(rrc) > 0 else self.NULL)\n",
    "\n",
    "                if self.use_pos:\n",
    "                    p_features.append(ex['pos'][lc[0]] if len(lc) > 0 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][rc[0]] if len(rc) > 0 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][lc[1]] if len(lc) > 1 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][rc[1]] if len(rc) > 1 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][llc[0]] if len(llc) > 0 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][rrc[0]] if len(rrc) > 0 else self.P_NULL)\n",
    "\n",
    "                if self.use_dep:\n",
    "                    l_features.append(ex['label'][lc[0]] if len(lc) > 0 else self.L_NULL)\n",
    "                    l_features.append(ex['label'][rc[0]] if len(rc) > 0 else self.L_NULL)\n",
    "                    l_features.append(ex['label'][lc[1]] if len(lc) > 1 else self.L_NULL)\n",
    "                    l_features.append(ex['label'][rc[1]] if len(rc) > 1 else self.L_NULL)\n",
    "                    l_features.append(ex['label'][llc[0]] if len(llc) > 0 else self.L_NULL)\n",
    "                    l_features.append(ex['label'][rrc[0]] if len(rrc) > 0 else self.L_NULL)\n",
    "            else:\n",
    "                features += [self.NULL] * 6\n",
    "                if self.use_pos:\n",
    "                    p_features += [self.P_NULL] * 6\n",
    "                if self.use_dep:\n",
    "                    l_features += [self.L_NULL] * 6\n",
    "\n",
    "        features += p_features + l_features\n",
    "        assert len(features) == self.n_features\n",
    "        return features\n",
    "\n",
    "    def get_oracle(self, stack, buf, ex):\n",
    "        if len(stack) < 2:\n",
    "            return self.n_trans - 1\n",
    "\n",
    "        i0 = stack[-1]\n",
    "        i1 = stack[-2]\n",
    "        h0 = ex['head'][i0]\n",
    "        h1 = ex['head'][i1]\n",
    "        l0 = ex['label'][i0]\n",
    "        l1 = ex['label'][i1]\n",
    "\n",
    "        if self.unlabeled:\n",
    "            if (i1 > 0) and (h1 == i0):\n",
    "                return 0\n",
    "            elif (i1 >= 0) and (h0 == i1) and \\\n",
    "                 (not any([x for x in buf if ex['head'][x] == i0])):\n",
    "                return 1\n",
    "            else:\n",
    "                return None if len(buf) == 0 else 2\n",
    "        else:\n",
    "            if (i1 > 0) and (h1 == i0):\n",
    "                return l1 if (l1 >= 0) and (l1 < self.n_deprel) else None\n",
    "            elif (i1 >= 0) and (h0 == i1) and \\\n",
    "                 (not any([x for x in buf if ex['head'][x] == i0])):\n",
    "                return l0 + self.n_deprel if (l0 >= 0) and (l0 < self.n_deprel) else None\n",
    "            else:\n",
    "                return None if len(buf) == 0 else self.n_trans - 1\n",
    "\n",
    "    def create_instances(self, examples):\n",
    "        all_instances = []\n",
    "        succ = 0\n",
    "        for id, ex in enumerate(examples):\n",
    "            n_words = len(ex['word']) - 1\n",
    "\n",
    "            # arcs = {(h, t, label)}\n",
    "            stack = [0]\n",
    "            buf = [i + 1 for i in range(n_words)]\n",
    "            arcs = []\n",
    "            instances = []\n",
    "            for i in range(n_words * 2):\n",
    "                gold_t = self.get_oracle(stack, buf, ex)\n",
    "                if gold_t is None:\n",
    "                    break\n",
    "                legal_labels = self.legal_labels(stack, buf)\n",
    "                assert legal_labels[gold_t] == 1\n",
    "                instances.append((self.extract_features(stack, buf, arcs, ex),\n",
    "                                  legal_labels, gold_t))\n",
    "                if gold_t == self.n_trans - 1:\n",
    "                    stack.append(buf[0])\n",
    "                    buf = buf[1:]\n",
    "                elif gold_t < self.n_deprel:\n",
    "                    arcs.append((stack[-1], stack[-2], gold_t))\n",
    "                    stack = stack[:-2] + [stack[-1]]\n",
    "                else:\n",
    "                    arcs.append((stack[-2], stack[-1], gold_t - self.n_deprel))\n",
    "                    stack = stack[:-1]\n",
    "            else:\n",
    "                succ += 1\n",
    "                all_instances += instances\n",
    "\n",
    "        return all_instances\n",
    "\n",
    "    def legal_labels(self, stack, buf):\n",
    "        labels = ([1] if len(stack) > 2 else [0]) * self.n_deprel\n",
    "        labels += ([1] if len(stack) >= 2 else [0]) * self.n_deprel\n",
    "        labels += [1] if len(buf) > 0 else [0]\n",
    "        return labels\n",
    "\n",
    "    def parse(self, dataset, eval_batch_size=5000):\n",
    "        sentences = []\n",
    "        sentence_id_to_idx = {}\n",
    "        for i, example in enumerate(dataset):\n",
    "            n_words = len(example['word']) - 1\n",
    "            sentence = [j + 1 for j in range(n_words)]\n",
    "            sentences.append(sentence)\n",
    "            sentence_id_to_idx[id(sentence)] = i\n",
    "\n",
    "        model = ModelWrapper(self, dataset, sentence_id_to_idx)\n",
    "        dependencies = minibatch_parse(sentences, model, eval_batch_size)\n",
    "\n",
    "        UAS = all_tokens = 0.0\n",
    "        with tqdm(total=len(dataset)) as prog:\n",
    "            for i, ex in enumerate(dataset):\n",
    "                head = [-1] * len(ex['word'])\n",
    "                for h, t, in dependencies[i]:\n",
    "                    head[t] = h\n",
    "                for pred_h, gold_h, gold_l, pos in \\\n",
    "                        zip(head[1:], ex['head'][1:], ex['label'][1:], ex['pos'][1:]):\n",
    "                        assert self.id2tok[pos].startswith(P_PREFIX)\n",
    "                        pos_str = self.id2tok[pos][len(P_PREFIX):]\n",
    "                        if (self.with_punct) or (not punct(self.language, pos_str)):\n",
    "                            UAS += 1 if pred_h == gold_h else 0\n",
    "                            all_tokens += 1\n",
    "                prog.update(i + 1)\n",
    "        UAS /= all_tokens\n",
    "        return UAS, dependencies\n",
    "\n",
    "\n",
    "class ModelWrapper(object):\n",
    "    def __init__(self, parser, dataset, sentence_id_to_idx):\n",
    "        self.parser = parser\n",
    "        self.dataset = dataset\n",
    "        self.sentence_id_to_idx = sentence_id_to_idx\n",
    "\n",
    "    def predict(self, partial_parses):\n",
    "        mb_x = [self.parser.extract_features(p.stack, p.buffer, p.dependencies,\n",
    "                                             self.dataset[self.sentence_id_to_idx[id(p.sentence)]])\n",
    "                for p in partial_parses]\n",
    "        mb_x = np.array(mb_x).astype('int32')\n",
    "        mb_x = torch.from_numpy(mb_x).long()\n",
    "        mb_l = [self.parser.legal_labels(p.stack, p.buffer) for p in partial_parses]\n",
    "\n",
    "        pred = self.parser.model(mb_x)\n",
    "        pred = pred.detach().numpy()\n",
    "        pred = np.argmax(pred + 10000 * np.array(mb_l).astype('float32'), 1)\n",
    "        pred = [\"S\" if p == 2 else (\"LA\" if p == 0 else \"RA\") for p in pred]\n",
    "        return pred\n",
    "\n",
    "\n",
    "def read_conll(in_file, lowercase=False, max_example=None):\n",
    "    examples = []\n",
    "    with open(in_file) as f:\n",
    "        word, pos, head, label = [], [], [], []\n",
    "        for line in f.readlines():\n",
    "            sp = line.strip().split('\\t')\n",
    "            if len(sp) == 10:\n",
    "                if '-' not in sp[0]:\n",
    "                    word.append(sp[1].lower() if lowercase else sp[1])\n",
    "                    pos.append(sp[4])\n",
    "                    head.append(int(sp[6]))\n",
    "                    label.append(sp[7])\n",
    "            elif len(word) > 0:\n",
    "                examples.append({'word': word, 'pos': pos, 'head': head, 'label': label})\n",
    "                word, pos, head, label = [], [], [], []\n",
    "                if (max_example is not None) and (len(examples) == max_example):\n",
    "                    break\n",
    "        if len(word) > 0:\n",
    "            examples.append({'word': word, 'pos': pos, 'head': head, 'label': label})\n",
    "    return examples\n",
    "\n",
    "\n",
    "def build_dict(keys, n_max=None, offset=0):\n",
    "    count = Counter()\n",
    "    for key in keys:\n",
    "        count[key] += 1\n",
    "    ls = count.most_common() if n_max is None \\\n",
    "        else count.most_common(n_max)\n",
    "\n",
    "    return {w[0]: index + offset for (index, w) in enumerate(ls)}\n",
    "\n",
    "\n",
    "def punct(language, pos):\n",
    "    if language == 'english':\n",
    "        return pos in [\"''\", \",\", \".\", \":\", \"``\", \"-LRB-\", \"-RRB-\"]\n",
    "    elif language == 'chinese':\n",
    "        return pos == 'PU'\n",
    "    elif language == 'french':\n",
    "        return pos == 'PUNC'\n",
    "    elif language == 'german':\n",
    "        return pos in [\"$.\", \"$,\", \"$[\"]\n",
    "    elif language == 'spanish':\n",
    "        # http://nlp.stanford.edu/software/spanish-faq.shtml\n",
    "        return pos in [\"f0\", \"faa\", \"fat\", \"fc\", \"fd\", \"fe\", \"fg\", \"fh\",\n",
    "                       \"fia\", \"fit\", \"fp\", \"fpa\", \"fpt\", \"fs\", \"ft\",\n",
    "                       \"fx\", \"fz\"]\n",
    "    elif language == 'universal':\n",
    "        return pos == 'PUNCT'\n",
    "    else:\n",
    "        raise ValueError('language: %s is not supported.' % language)\n",
    "\n",
    "\n",
    "def minibatches(data, batch_size):\n",
    "    x = np.array([d[0] for d in data])\n",
    "    y = np.array([d[2] for d in data])\n",
    "    one_hot = np.zeros((y.size, 3))\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return get_minibatches([x, one_hot], batch_size)\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(reduced=True):\n",
    "    config = Config()\n",
    "\n",
    "    print(\"Loading data...\",)\n",
    "    start = time.time()\n",
    "    train_set = read_conll(os.path.join(config.data_path, config.train_file),\n",
    "                           lowercase=config.lowercase)\n",
    "    dev_set = read_conll(os.path.join(config.data_path, config.dev_file),\n",
    "                         lowercase=config.lowercase)\n",
    "    test_set = read_conll(os.path.join(config.data_path, config.test_file),\n",
    "                          lowercase=config.lowercase)\n",
    "    if reduced:\n",
    "        train_set = train_set[:1000]\n",
    "        dev_set = dev_set[:500]\n",
    "        test_set = test_set[:500]\n",
    "    print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "    print(\"Building parser...\",)\n",
    "    start = time.time()\n",
    "    parser = Parser(train_set)\n",
    "    print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "    print(\"Loading pretrained embeddings...\",)\n",
    "    start = time.time()\n",
    "    word_vectors = {}\n",
    "    for line in open(config.embedding_file).readlines():\n",
    "        sp = line.strip().split()\n",
    "        word_vectors[sp[0]] = [float(x) for x in sp[1:]]\n",
    "    embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
    "\n",
    "    for token in parser.tok2id:\n",
    "        i = parser.tok2id[token]\n",
    "        if token in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token]\n",
    "        elif token.lower() in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token.lower()]\n",
    "    print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "    print(\"Vectorizing data...\",)\n",
    "    start = time.time()\n",
    "    train_set = parser.vectorize(train_set)\n",
    "    dev_set = parser.vectorize(dev_set)\n",
    "    test_set = parser.vectorize(test_set)\n",
    "    print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "    print(\"Preprocessing training data...\",)\n",
    "    start = time.time()\n",
    "    train_examples = parser.create_instances(train_set)\n",
    "    print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "    return parser, embeddings_matrix, train_examples, dev_set, test_set,\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here is the skeleton code to implement this network using PyTorch.  Complete the <code>\\_\\_init\\_\\_</code>, <code>embedding_lookup</code> and <code>forward</code> functions to implement the model. \n",
    "\n",
    "Please DO NOT use <code>torch.nn.Linear</code> or <code>torch.nn.Embedding</code>.  We are basically asking you to implement the Linear layer and Embedding layer by yourself so you can adjust the code according to the equation we got.\n",
    "\n",
    "Please also follow the naming requirements in our TODO to avoid any problems.  \n",
    "\n",
    "**Hints**\n",
    "- Each of the variable (<code>self.embed_to_hidden_weight, self.embed_to_hidden_bias, self.hidden_to_logits_weight, self.hidden_to_logits_bias</code>) corresponds to ($\\mathbf{W}, \\mathbf{b}_1, \\mathbf{U}, \\mathbf{b}_2$)\n",
    "- It may help to work backwards in the algorithm (start from $\\hat{\\mathbf{y}}$) and keep track of the matrix/vector shapes\n",
    "- At worst, loss should be smaller than 0.08, and UAS larger than 87 on the dev set (around :-)). The original paper got around 92.5 UAS.\n",
    "- Should take around 1 hour to train the model on the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParserModel(nn.Module):\n",
    "    \"\"\" Feedforward neural network with an embedding layer and single hidden layer.\n",
    "    The ParserModel will predict which transition should be applied to a\n",
    "    given partial parse configuration.\n",
    "    PyTorch Notes:\n",
    "        - Note that \"ParserModel\" is a subclass of the \"nn.Module\" class. In \n",
    "          PyTorch all neural networks\n",
    "            are a subclass of this \"nn.Module\".\n",
    "        - The \"__init__\" method is where you define all the layers and their \n",
    "          respective parameters (embedding layers, linear layers, dropout layers, etc.).\n",
    "        - \"__init__\" gets automatically called when you create a new instance \n",
    "          of your class, e.g. when you write \"m = ParserModel()\".\n",
    "        - Other methods of ParserModel can access variables that have \"self.\" \n",
    "          prefix. Thus, you should add the \"self.\" prefix layers, values, etc. \n",
    "          that you want to utilize in other ParserModel methods.\n",
    "        - For further documentation on \"nn.Module\" please see\n",
    "          https://pytorch.org/docs/stable/nn.html.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embeddings, n_features=36,\n",
    "                 hidden_size=400, n_classes=3, dropout_prob=0.5):\n",
    "        \"\"\" Initialize the parser model.\n",
    "        @param embeddings (Tensor): word embeddings (num_words, embedding_size)\n",
    "        @param n_features (int): number of input features\n",
    "        @param hidden_size (int): number of hidden units\n",
    "        @param n_classes (int): number of output classes\n",
    "        @param dropout_prob (float): dropout probability\n",
    "        \"\"\"\n",
    "        super(ParserModel, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.embed_size = embeddings.shape[1]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pretrained_embeddings = nn.Embedding(\n",
    "            embeddings.shape[0], self.embed_size)\n",
    "        self.pretrained_embeddings.weight = nn.Parameter(\n",
    "            torch.tensor(embeddings))\n",
    "\n",
    "        # YOUR CODE HERE (~5 Lines)\n",
    "        # TODO:\n",
    "        # 1) Construct `self.embed_to_hidden` linear layer, initializing the weight matrix\n",
    "        # with the `nn.init.xavier_uniform_` function with `gain = 1` (default)\n",
    "        # 2) Construct `self.dropout` layer.\n",
    "        # 3) Construct `self.hidden_to_logits` linear layer, initializing the weight matrix\n",
    "        # with the `nn.init.xavier_uniform_` function with `gain = 1` (default)\n",
    "        ###\n",
    "        # Note: Here, we use Xavier Uniform Initialization for our Weight initialization.\n",
    "        # It has been shown empirically, that this provides better initial weights\n",
    "        # for training networks than random uniform initialization.\n",
    "        # For more details checkout this great blogpost:\n",
    "        # http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
    "        # Hints:\n",
    "        # - After you create a linear layer you can access the weight\n",
    "        # matrix via:\n",
    "        # linear_layer.weight\n",
    "        ###\n",
    "        # Please see the following docs for support:\n",
    "        # Linear Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n",
    "        # Xavier Init: https://pytorch.org/docs/stable/nn.html#torch.nn.init.xavier_uniform_\n",
    "        # Dropout: https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout\n",
    "\n",
    "        self.embed_to_hidden = torch.nn.Linear(self.embed_size * self.n_features, \n",
    "                                               self.hidden_size, bias=True)\n",
    "        torch.nn.init.xavier_uniform_(self.embed_to_hidden.weight, gain=1)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p=self.dropout_prob)\n",
    "\n",
    "        self.hidden_to_logits = torch.nn.Linear(self.hidden_size, self.n_classes)\n",
    "        torch.nn.init.xavier_uniform_(self.hidden_to_logits.weight, gain=1)       \n",
    "        \n",
    "\n",
    "        # END YOUR CODE\n",
    "\n",
    "    def embedding_lookup(self, t):\n",
    "        \"\"\" Utilize `self.pretrained_embeddings` to map input `t` from input tokens\n",
    "            (integers) to embedding vectors.\n",
    "            PyTorch Notes:\n",
    "                - `self.pretrained_embeddings` is a torch.nn.Embedding object \n",
    "                  that we defined in __init__\n",
    "                - Here `t` is a tensor where each row represents a list of \n",
    "                  features. Each feature is represented by an integer (input token).\n",
    "                - In PyTorch the Embedding object, e.g. \n",
    "                  `self.pretrained_embeddings`, allows you to\n",
    "                  go from an index to embedding. Please see the documentation\n",
    "                  (https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding)\n",
    "                  to learn how to use `self.pretrained_embeddings` to extract \n",
    "                  the embeddings for your tensor `t`.\n",
    "            @param t (Tensor): input tensor of tokens (batch_size, n_features)\n",
    "            @return x (Tensor): tensor of embeddings for words represented in t\n",
    "                                (batch_size, n_features * embed_size)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE (~1-3 Lines)\n",
    "        # TODO:\n",
    "        # 1) Use `self.pretrained_embeddings` to lookup the embeddings for the\n",
    "        # input tokens in `t`.\n",
    "        # 2) After you apply the embedding lookup, you will have a tensor shape\n",
    "        # (batch_size, n_features, embedding_size).\n",
    "        # Use the tensor `view` or `reshape` method to reshape the embeddings tensor to\n",
    "        # (batch_size, n_features * embedding_size)\n",
    "        ###\n",
    "        # Note: In order to get batch_size, you may need use the tensor .size()\n",
    "        # function:\n",
    "        # https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size\n",
    "        ###\n",
    "        # Please see the following docs for support:\n",
    "        # Embedding Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n",
    "        # View: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n",
    "\n",
    "        embeddings = self.pretrained_embeddings.forward(t)\n",
    "        x = embeddings.view(t.shape[0], self.embed_size * self.n_features)      \n",
    "\n",
    "        # END YOUR CODE\n",
    "        return x\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\" Run the model forward.\n",
    "            Note that we will not apply the softmax function here because it is \n",
    "            included in the loss function nn.CrossEntropyLoss\n",
    "            PyTorch Notes:\n",
    "                - Every nn.Module object (PyTorch model) has a `forward` \n",
    "                  function.\n",
    "                - When you apply your nn.Module to an input tensor `t` this \n",
    "                  function is applied to the tensor.\n",
    "                  For example, if you created an instance of your ParserModel \n",
    "                  and applied it to some `t` as follows, the `forward` function \n",
    "                  would called on `t` and the result would be stored in the \n",
    "                  `output` variable:\n",
    "                        model = ParserModel()\n",
    "                        output = model(t) # this calls the forward function\n",
    "                # torch.nn.Module.forward\n",
    "                - For more details checkout: https://pytorch.org/docs/stable/nn.html\n",
    "        @param t (Tensor): input tensor of tokens (batch_size, n_features)\n",
    "        @return logits (Tensor): tensor of predictions (output after applying \n",
    "                                 the layers of the network) without applying \n",
    "                                 softmax (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE (~3-5 lines)\n",
    "        # TODO:\n",
    "        # 1) Apply `self.embedding_lookup` to `t` to get the embeddings\n",
    "        # 2) Apply `embed_to_hidden` linear layer to the embeddings\n",
    "        # 3) Apply relu non-linearity to the output of step 2 to get the hidden units.\n",
    "        # 4) Apply dropout layer to the output of step 3.\n",
    "        # 5) Apply `hidden_to_logits` layer to the output of step 4 to get the logits.\n",
    "        ###\n",
    "        # Note: We do not apply the softmax to the logits here, because\n",
    "        # the loss function (torch.nn.CrossEntropyLoss) applies it more efficiently.\n",
    "        ###\n",
    "        # Please see the following docs for support:\n",
    "        # ReLU: https://pytorch.org/docs/stable/nn.html?highlight=relu#torch.nn.functional.relu\n",
    "        \n",
    "        embed = self.embedding_lookup(t)\n",
    "        hidden = F.relu(self.embed_to_hidden.forward(embed))\n",
    "        dropout = self.dropout.forward(hidden)\n",
    "        logits = self.hidden_to_logits.forward(dropout)     \n",
    "        \n",
    "        # END YOUR CODE\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now complete the <code>train_for_epoch</code> and <code>train</code> functions to actually train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Primary Functions\n",
    "# -----------------\n",
    "\n",
    "\n",
    "def train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005):\n",
    "    \"\"\" Train the neural dependency parser.\n",
    "    @param parser (Parser): Neural Dependency Parser\n",
    "    @param train_data ():\n",
    "    @param dev_data ():\n",
    "    @param output_path (str): Path to which model weights and results are written.\n",
    "    @param batch_size (int): Number of examples in a single batch\n",
    "    @param n_epochs (int): Number of training epochs\n",
    "    @param lr (float): Learning rate\n",
    "    \"\"\"\n",
    "    best_dev_UAS = 0\n",
    "\n",
    "    # YOUR CODE HERE (2 lines)\n",
    "    # TODO:\n",
    "    # 1) Construct Adam Optimizer in variable `optimizer`\n",
    "    # 2) Construct the Cross Entropy Loss Function in variable `loss_func`\n",
    "    ###\n",
    "    # Hint: Use `parser.model.parameters()` to pass optimizer\n",
    "    # necessary parameters to tune.\n",
    "    # Please see the following docs for support:\n",
    "    # Adam Optimizer: https://pytorch.org/docs/stable/optim.html\n",
    "    # Cross Entropy Loss: https://pytorch.org/docs/stable/nn.html#crossentropyloss\n",
    "\n",
    "    \n",
    "    optimizer = torch.optim.Adam(parser.model.parameters(), lr=lr)\n",
    "    loss_func = torch.nn.CrossEntropyLoss()  \n",
    "\n",
    "    # END YOUR CODE\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
    "        dev_UAS = train_for_epoch(\n",
    "            parser, train_data, dev_data, optimizer, loss_func, batch_size)\n",
    "        if dev_UAS > best_dev_UAS:\n",
    "            best_dev_UAS = dev_UAS\n",
    "            print(\"New best dev UAS! Saving model.\")\n",
    "            torch.save(parser.model.state_dict(), output_path)\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "def train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size):\n",
    "    \"\"\" Train the neural dependency parser for single epoch.\n",
    "    Note: In PyTorch we can signify train versus test and automatically have\n",
    "    the Dropout Layer applied and removed, accordingly, by specifying\n",
    "    whether we are training, `model.train()`, or evaluating, `model.eval()`\n",
    "    @param parser (Parser): Neural Dependency Parser\n",
    "    @param train_data ():\n",
    "    @param dev_data ():\n",
    "    @param optimizer (nn.Optimizer): Adam Optimizer\n",
    "    @param loss_func (nn.CrossEntropyLoss): Cross Entropy Loss Function\n",
    "    @param batch_size (int): batch size\n",
    "    @param lr (float): learning rate\n",
    "    @return dev_UAS (float): Unlabeled Attachment Score (UAS) for dev data\n",
    "    \"\"\"\n",
    "    parser.model.train()  # Places model in \"train\" mode, i.e. apply dropout layer\n",
    "    n_minibatches = math.ceil(len(train_data) / batch_size)\n",
    "    loss_meter = AverageMeter()\n",
    "\n",
    "    with tqdm(total=(n_minibatches)) as prog:\n",
    "        for i, (train_x, train_y) in enumerate(minibatches(train_data, batch_size)):\n",
    "            optimizer.zero_grad()   # remove any baggage in the optimizer\n",
    "            loss = 0.  # store loss for this batch here\n",
    "            train_x = torch.from_numpy(train_x).long()\n",
    "            train_y = torch.from_numpy(train_y.nonzero()[1]).long()\n",
    "\n",
    "            # YOUR CODE HERE (~5-10 lines)\n",
    "            # TODO:\n",
    "            # 1) Run train_x forward through model to produce `logits`\n",
    "            # 2) Use the `loss_func` parameter to apply the PyTorch CrossEntropyLoss function.\n",
    "            # This will take `logits` and `train_y` as inputs. It will output the CrossEntropyLoss\n",
    "            # between softmax(`logits`) and `train_y`. Remember that softmax(`logits`)\n",
    "            # are the predictions (y^ from the PDF).\n",
    "            # 3) Backprop losses\n",
    "            # 4) Take step with the optimizer\n",
    "            # Please see the following docs for support:\n",
    "            # Optimizer Step: https://pytorch.org/docs/stable/optim.html#optimizer-step\n",
    "            \n",
    "            logits = parser.model.forward(train_x)\n",
    "            loss = loss_func(logits, train_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()          \n",
    "            \n",
    "            \n",
    "            # END YOUR CODE\n",
    "            prog.update(1)\n",
    "            loss_meter.update(loss.item())\n",
    "\n",
    "    print(\"Average Train Loss: {}\".format(loss_meter.avg))\n",
    "\n",
    "    print(\"Evaluating on dev set\",)\n",
    "    parser.model.eval()  # Places model in \"eval\" mode, i.e. don't apply dropout layer\n",
    "    dev_UAS, _ = parser.parse(dev_data)\n",
    "    print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
    "    return dev_UAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Evaluation\n",
    "\n",
    "Now execute this code to actually train your model and compute predictions on test data from Penn Treebank (annotated with Universal Dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INITIALIZING\n",
      "================================================================================\n",
      "Loading data...\n",
      "took 2.57 seconds\n",
      "Building parser...\n",
      "took 1.32 seconds\n",
      "Loading pretrained embeddings...\n",
      "took 3.06 seconds\n",
      "Vectorizing data...\n",
      "took 1.94 seconds\n",
      "Preprocessing training data...\n",
      "took 59.02 seconds\n",
      "took 0.07 seconds\n",
      "\n",
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1848/1848 [00:58<00:00, 31.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.1684717121594525\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1445850it [00:00, 27280202.42it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 84.96\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1848/1848 [00:57<00:00, 32.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.10371681396321301\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1445850it [00:00, 26477069.34it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 87.09\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1848/1848 [00:56<00:00, 32.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.08936655070447239\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1445850it [00:00, 26302516.20it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 87.83\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1848/1848 [00:57<00:00, 31.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.0805013144683109\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1445850it [00:00, 26206583.43it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 88.52\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1848/1848 [00:59<00:00, 31.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.07377310011140777\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1445850it [00:00, 26293050.93it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 88.70\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1848/1848 [01:01<00:00, 30.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.06834021272758643\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1445850it [00:00, 27044790.19it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 88.94\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1848/1848 [00:59<00:00, 30.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.06408111364641167\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1445850it [00:00, 26939427.12it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 89.03\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1848/1848 [00:54<00:00, 33.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.05995714332851948\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1445850it [00:00, 25977795.17it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 89.05\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1848/1848 [00:57<00:00, 32.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.056440737980389684\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1445850it [00:00, 26208848.62it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 89.22\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 10 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1848/1848 [01:00<00:00, 30.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.05346750089640677\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1445850it [00:00, 26646459.29it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 89.39\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "================================================================================\n",
      "TESTING\n",
      "================================================================================\n",
      "Restoring the best model weights found on the dev set\n",
      "Final evaluation on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2919736it [00:00, 38942539.46it/s]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- test UAS: 89.87\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Note: Set debug to False, when training on entire corpus\n",
    "# debug = True\n",
    "debug = False\n",
    "\n",
    "print(80 * \"=\")\n",
    "print(\"INITIALIZING\")\n",
    "print(80 * \"=\")\n",
    "parser, embeddings, train_data, dev_data, test_data = load_and_preprocess_data(\n",
    "    debug)\n",
    "\n",
    "start = time.time()\n",
    "model = ParserModel(embeddings)\n",
    "parser.model = model\n",
    "print(\"took {:.2f} seconds\\n\".format(time.time() - start))\n",
    "\n",
    "print(80 * \"=\")\n",
    "print(\"TRAINING\")\n",
    "print(80 * \"=\")\n",
    "output_dir = \"results/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n",
    "output_path = output_dir + \"model.weights\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "train(parser, train_data, dev_data, output_path,\n",
    "      batch_size=1024, n_epochs=10, lr=0.0005)\n",
    "\n",
    "if not debug:\n",
    "    print(80 * \"=\")\n",
    "    print(\"TESTING\")\n",
    "    print(80 * \"=\")\n",
    "    print(\"Restoring the best model weights found on the dev set\")\n",
    "    parser.model.load_state_dict(torch.load(output_path))\n",
    "    print(\"Final evaluation on test set\",)\n",
    "    parser.model.eval()\n",
    "    UAS, dependencies = parser.parse(test_data)\n",
    "    print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
